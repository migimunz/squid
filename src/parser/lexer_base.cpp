#include "lexer_base.hpp"
#include <stdexcept>
#include <string>

namespace squid
{

lexer_base::lexer_base(const str_iter &iter)
	:token_matchers(),
	 token_names(),
	 always_match_list(),
	 beginning(iter),
	 space_indent("    ")
{
	init();
}

void lexer_base::register_token(
	token_type type, 
	token_match_func func, 
	const std::string &name,
	bool always_match)
{
	if(!valid_token_type(type))
		return;
	token_names[type] = name;
	token_matchers[type] = func;
	if(always_match)
		always_match_list.push_back(type);
}

void lexer_base::init()
{
	//FIXME: Ubaci newline
	register_token(INVALID, 	match_invalid, 		"INVALID TOKEN",	false);
	register_token(END_OF_TEXT, match_end_of_text, 	"END_OF_TEXT",		true);
	register_token(INDENT_FRAG, match_indent_frag, 	"INDENT FRAGMENT",	false);
	register_token(NEWLINE, 	match_newline, 		"NEWLINE",			false);
	register_token(WHITESPACE,  match_whitespace,	"WHITESPACE",		true);
	/** OPERATORS AND SYMBOLS **/
	register_token(ARROW, 		match_arrow, 		"'->'", 			true);
	register_token(PLUS, 		match_plus, 		"'+'", 				true);
	register_token(MINUS,		match_minus, 		"'-'", 				true);
	register_token(MULTIPLY,	match_multiply, 	"'*'", 				true);
	register_token(DIVIDE,		match_divide, 		"'/'", 				true);
	register_token(MATCH,		match_match, 		"'='", 				true);
	register_token(PAREN_OPEN,	match_paren_open,	"'('", 				true);
	register_token(PAREN_CLOSED,match_paren_closed,	"')'", 				true);

	/** KEYWORDS AND LITERALS **/
	register_token(DEF, 		match_def, 			"'def'", 			true);
	register_token(AND, 		match_and, 			"'and'", 			true);
	register_token(OR, 			match_or, 			"'or'", 			true);
	register_token(NOT, 		match_not, 			"'not'", 			true);
	register_token(COMMA, 		match_comma, 		"','", 				true);
	register_token(FLOAT, 		match_float,		"FLOAT",			true);
	register_token(HEX_INTEGER, match_hex_integer,	"HEX_INTEGER",		true);
	register_token(INTEGER, 	match_integer,		"INTEGER",			true);
	register_token(DOT, 		match_dot, 			"'.'", 				true);
	register_token(IDENTIFIER, 	match_identifier, 	"IDENTIFIER", 		true);
	/**
	 * INDENT and DEDENT are never matched in text, only
	 * generated by the lexer when matching INDENT FRAGMENT
	 */
	register_token(INDENT,		match_always_fail,	"INDENT",			false);
	register_token(DEDENT,		match_always_fail,	"DEDENT",			false);
}

bool lexer_base::valid_token_type(token_type type) const
{
	return type >= INVALID && type < TOKEN_TYPE_COUNT;
}

str_iter lexer_base::match_single_token(const str_iter &current, token &t, token_type type) const
{
	token_match_func matcher = get_token_matcher(current, type);
	string match = matcher(current);
	if(match.valid())
	{
		t = token(type, match, str_iter::dist(beginning, current));
		return match.get_end_iter();
	}
	else
	{
		return str_iter();
	}
}

str_iter lexer_base::match_any_token(const str_iter &current, token &t) const
{
	for(auto iter = always_match_list.begin(); iter != always_match_list.end(); ++iter)
	{
		token_type type = (token_type)*iter;
		str_iter new_iter = match_single_token(current, t, type);
		if(new_iter.valid())
			return new_iter;
	}
	t = token(INVALID);
	return str_iter();
}

std::string lexer_base::get_token_name(const str_iter &current, token_type type) const
{
	if(!valid_token_type(type))
		throw lexer_exception("Invalid token type in get_token_name.", current);
	else
		return token_names.at(type);
}

token_match_func lexer_base::get_token_matcher(const str_iter &current, token_type type) const
{
	if(!valid_token_type(type))
		throw lexer_exception("Invalid token type in get_token_matcher.", current);
	else
		return token_matchers.at(type);
}

//FIXME: preimenuj u is_token_skippable
// bool lexer_base::is_token_whitespace(skip_pattern sp, const token &tok)
// {
// 	if(tok.type == NEWLINE && (sp & SKIP_NEWLINE))
// 		return true;
// 	else if(tok.type == INDENT_FRAG && (sp & SKIP_INDENT))
// 		return true;
// 	else if(tok.type == WHITESPACE)
// 		return true;
// 	else
// 		return false;
// }


token_iterator::token_iterator(lexer_base &lexer, int indent_level)
	:lexer(lexer),
	 current(lexer.beginning),
	 indent_level(indent_level)
{
}

token_iterator::token_iterator(const token_iterator &rhs)
	:lexer(rhs.lexer),
	 current(rhs.current),
	 indent_level(rhs.indent_level)
{
}

token_iterator &token_iterator::operator=(const token_iterator &rhs)
{
	if(this != &rhs)
	{
		if(&lexer != &(rhs.lexer))
		{
			throw std::runtime_error("Token iterators not from same stream.");
		}
		current = rhs.current;
	}
	return *this;
}

int token_iterator::compute_indent_level(const string &indent_str) const
{
	str_iter iter = indent_str.get_start_iter();
	str_iter end  = indent_str.get_end_iter();
	int level = 0;
	iter = iter.match("\n");
	while(iter.valid() && iter < end)
	{

		iter = iter.match(lexer.space_indent) | iter.match("\t");
		if(iter.valid())
			level++;
		else
			throw lexer_exception("Malformed indentation detected.", current);
	}
	return level;
}


int token_iterator::get_position() const
{
	return str_iter::dist(lexer.beginning, current);
}

bool token_iterator::peek(token &tok_out) const
{
	str_iter iter = current;
	token tok;
	do
	{
		iter = lexer.match_any_token(iter, tok);
	} while(tok.type == WHITESPACE && tok.valid());
	tok_out = tok;
	return tok.valid();
}

token token_iterator::peek() const
{
	token tok;
	peek(tok);
	return tok;
}

bool token_iterator::next(token &tok_out)
{
	if(peek(tok_out))
	{
		current = tok_out.text.get_end_iter();
		return true;
	}
	else
	{
		tok_out = token(INVALID, string(current, current), get_position());
		return false;
	}

}

token token_iterator::next()
{
	token tok;
	next(tok);
	return tok;
}

void token_iterator::consume_expected(token_type type)
{
	token tok;
	if(peek(tok) && tok.type == type)
	{
		current = tok.text.get_end_iter();
	}
	else
	{
		throw token_expectation_exception(tok.type, type);
	}
}

bool token_iterator::peek_newline() const
{
	token t;
	str_iter iter = lexer.match_single_token(current, t, NEWLINE);
	return iter.valid();
}

void token_iterator::consume_newline()
{
	token t;
	str_iter iter = lexer.match_single_token(current, t, NEWLINE);
	if(iter.valid())
	{
		current = iter;
	}
	else
	{
		throw lexer_exception("Expected NEWLINE.", current);
	}
}


bool token_iterator::peek_indentation(token &tok) const
{
	str_iter iter = lexer.match_single_token(current, tok, INDENT_FRAG);
	if(!iter.valid())
	{
		return false;
	}

	int new_indent = compute_indent_level(tok.text);
	int indent_delta = new_indent - indent_level;
	if(indent_delta > 0) //EMIT INDENT TOKEN
	{
		tok = token(INDENT, tok.text, tok.position);
		//FIXME: check for multiple indents
	}
	else if(indent_delta < 0)
	{
		string tok_str;
		if(indent_delta < -1) //If more than one dedent, stay on same line
			tok_str = string(tok.text.get_start_iter(), tok.text.get_start_iter());
		else //If not, skip ahead.
			tok_str = tok.text;
		tok = token(DEDENT, tok_str, tok.position);
	}
	else
	{
		tok = token(INDENT_UNCHANGED, tok.text, tok.position);
	}
	return true;
}

bool token_iterator::peek_indentation(token_type type) const
{
	token tok;
	return peek_indentation(tok) && tok.type == type;
}

bool token_iterator::consume_indentation(token &tok)
{
	if(peek_indentation(tok))
	{
		current = tok.text.get_end_iter();
		if(tok.type == INDENT)
		{
			indent_level++;
		}
		else if(tok.type == DEDENT)
		{
			indent_level--;
		}
		return true;
	}
	else
	{
		token(INVALID, string(current, current), get_position());
		return false;
	}
}

token token_iterator::consume_indentation()
{
	token tok;
	if(consume_indentation(tok))
	{
		return tok;
	}
	else
	{
		throw lexer_exception("Expected indentation", current);
	}
}

}